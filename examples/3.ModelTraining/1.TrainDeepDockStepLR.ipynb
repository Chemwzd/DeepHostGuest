{
 "cells": [
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "\"\"\"\n",
    "\n",
    "\"\"\"\n",
    "import os\n",
    "import json\n",
    "from torch_geometric.data import DataLoader\n",
    "from datetime import datetime\n",
    "import random\n",
    "import pandas as pd\n",
    "from DeepHostGuest.utils.data import *\n",
    "from DeepHostGuest.models import *\n",
    "from torch.optim.lr_scheduler import MultiStepLR\n",
    "from sklearn.model_selection import KFold\n",
    "import shutil\n",
    "import numpy as np\n",
    "\n",
    "# set the random seeds for reproducibility\n",
    "random.seed(1000)\n",
    "np.random.seed(1000)\n",
    "torch.cuda.manual_seed_all(1000)\n",
    "torch.manual_seed(1000)\n",
    "\n",
    "aug_fold = 10\n",
    "\n",
    "host_ply_dir = '/path/to/host_ply'\n",
    "host_mol_dir = '/path/to/mol'\n",
    "guest_mol_dir = '/path/to/guest_mol'\n",
    "train_dir = '/path/to/training'\n",
    "\n",
    "os.makedirs(train_dir, exist_ok=True)\n",
    "\n",
    "# The JSON file contains analysis data for 1,499 entries.\n",
    "with open('./analysis_info.json', 'r') as f:\n",
    "    check_info = json.load(f)\n",
    "\n",
    "prefixes = [i for i in check_info.keys()]\n",
    "print(len(prefixes))\n",
    "random.shuffle(prefixes)\n",
    "%matplotlib inline"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Prepare the training directories and files\n",
    "valid_names = prefixes[-99:]\n",
    "\n",
    "os.makedirs(train_dir, exist_ok=True)\n",
    "os.makedirs(os.path.join(train_dir, 'host_ply'), exist_ok=True)\n",
    "os.makedirs(os.path.join(train_dir, 'guest_mol'), exist_ok=True)\n",
    "os.makedirs(os.path.join(train_dir, 'train_model'), exist_ok=True)\n",
    "os.makedirs(os.path.join(train_dir, 'valid'), exist_ok=True)\n",
    "os.makedirs(os.path.join(train_dir, 'valid', 'aug_structures'), exist_ok=True)\n",
    "os.makedirs(os.path.join(train_dir, 'valid', 'structures'), exist_ok=True)\n",
    "\n",
    "for prefix in prefixes:\n",
    "    if prefix in valid_names:\n",
    "        for i in range(aug_fold):\n",
    "            shutil.copy(os.path.join(host_ply_dir, f'{prefix}_1_{i}.ply'),\n",
    "                        os.path.join(train_dir, 'valid', 'aug_structures'))\n",
    "            shutil.copy(os.path.join(guest_mol_dir, f'{prefix}_2_{i}.mol'),\n",
    "                        os.path.join(train_dir, 'valid', 'aug_structures'))\n",
    "        shutil.copy(os.path.join(host_ply_dir, f'{prefix}_1_0.ply'),\n",
    "                    os.path.join(train_dir, 'valid', 'structures', f'{prefix}_1.ply'))\n",
    "        shutil.copy(os.path.join(host_mol_dir, f'{prefix}_1_0.mol'),\n",
    "                    os.path.join(train_dir, 'valid', 'structures', f'{prefix}_1.mol'))\n",
    "        shutil.copy(os.path.join(guest_mol_dir, f'{prefix}_2_0.mol'),\n",
    "                    os.path.join(train_dir, 'valid', 'structures', f'{prefix}_2.mol'))\n",
    "    else:\n",
    "        for i in range(aug_fold):\n",
    "            shutil.copy(os.path.join(host_ply_dir, f'{prefix}_1_{i}.ply'),\n",
    "                        os.path.join(train_dir, 'host_ply'))\n",
    "            shutil.copy(os.path.join(guest_mol_dir, f'{prefix}_2_{i}.mol'),\n",
    "                        os.path.join(train_dir, 'guest_mol'))"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "removeHs = False\n",
    "\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "if removeHs:\n",
    "    guest_model = LigandNet(13, edge_features=7, residual_layers=10, dropout_rate=0.10)\n",
    "else:\n",
    "    guest_model = LigandNet(14, edge_features=7, residual_layers=10, dropout_rate=0.10)\n",
    "host_model = TargetNet(1, residual_layers=10, dropout_rate=0.10)\n",
    "model = DeepDock(guest_model, host_model, hidden_dim=64, n_gaussians=10, dropout_rate=0.10, dist_threhold=10.).to(\n",
    "    device)\n",
    "\n",
    "lr = 0.001\n",
    "epochs = 300\n",
    "if removeHs:\n",
    "    batch_size = 32\n",
    "else:\n",
    "    batch_size = 16\n",
    "save_each = 25\n",
    "aux_weight = 0.001\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "scheduler = MultiStepLR(optimizer, milestones=[50, 100, 150, 200, 250, 300, 350, 400, 450, 500], gamma=0.2)\n",
    "losses = []"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "%%time\n",
    "\n",
    "kfold = KFold(n_splits=10, shuffle=True, random_state=1000)\n",
    "training_set_path = os.path.join(train_dir, 'host_ply')\n",
    "training_names = [file for file in os.listdir(training_set_path) if file.endswith('.ply')]\n",
    "\n",
    "n = len(training_names)\n",
    "\n",
    "# train: test = 9 : 1\n",
    "train_index, test_index = next(iter(kfold.split(np.arange(n))))\n",
    "\n",
    "print(len(train_index), len(test_index))\n",
    "\n",
    "db_complex = HostGuest_dataset(removeHs=removeHs,\n",
    "                               root=train_dir)\n",
    "\n",
    "db_complex_train = [db_complex[i] for i in train_index]\n",
    "db_complex_test = [db_complex[i] for i in test_index]\n",
    "print('Complexes in training set:', len(db_complex_train))\n",
    "print('Complexes in test set:', len(db_complex_test))\n",
    "loader_train = DataLoader(db_complex_train, batch_size=batch_size, shuffle=True)\n",
    "loader_test = DataLoader(db_complex_test, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "now = datetime.now()\n",
    "print(now.strftime(\"Start date: %d/%m/%Y at %H:%M:%S\"))\n",
    "# format: {dataset_name}_{epochs}_{batch_size}_{lr}_{aux_weight}\n",
    "model_name = f'dist10_data1400_{removeHs}_{epochs}_{batch_size}_{lr}_{aux_weight}_mlr'\n",
    "os.chdir(os.path.join(train_dir, 'train_model'))\n",
    "\n",
    "\n",
    "def train():\n",
    "    model.train()\n",
    "\n",
    "    total_loss = 0\n",
    "    mdn_loss = 0\n",
    "    atom_loss = 0\n",
    "    bond_loss = 0\n",
    "    for data in loader_train:\n",
    "        optimizer.zero_grad()\n",
    "        target, ligand = data\n",
    "        target, ligand = target.to(device), ligand.to(device)\n",
    "        atom_labels = torch.argmax(ligand.x, dim=1, keepdim=False)\n",
    "        bond_labels = torch.argmax(ligand.edge_attr, dim=1, keepdim=False)\n",
    "\n",
    "        pi, sigma, mu, dist, atom_types, bond_types, batch = model(ligand, target)\n",
    "\n",
    "        mdn = mdn_loss_fn(pi, sigma, mu, dist)\n",
    "        mdn = mdn[torch.where(dist <= model.dist_threhold)[0]]\n",
    "        mdn = mdn.mean()\n",
    "        atom = F.cross_entropy(atom_types, atom_labels)\n",
    "        bond = F.cross_entropy(bond_types, bond_labels)\n",
    "        loss = mdn + (atom * aux_weight) + (bond * aux_weight)\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += loss.item() * (ligand.batch.max().item() + 1)\n",
    "        mdn_loss += mdn.item() * (ligand.batch.max().item() + 1)\n",
    "        atom_loss += atom.item() * (ligand.batch.max().item() + 1)\n",
    "        bond_loss += bond.item() * (ligand.batch.max().item() + 1)\n",
    "\n",
    "        #print('Step, Total Loss: {:.3f}, MDN: {:.3f}'.format(total_loss, mdn_loss))\n",
    "        if np.isinf(mdn_loss) or np.isnan(mdn_loss): break\n",
    "\n",
    "    return total_loss / len(loader_train.dataset), mdn_loss / len(loader_train.dataset), atom_loss / len(\n",
    "        loader_train.dataset), bond_loss / len(loader_train.dataset)\n",
    "\n",
    "\n",
    "@torch.no_grad()\n",
    "def test(dataset):\n",
    "    model.eval()\n",
    "\n",
    "    loader = DataLoader(dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "    total_loss = 0\n",
    "    mdn_loss = 0\n",
    "    atom_loss = 0\n",
    "    bond_loss = 0\n",
    "    for data in loader:\n",
    "        target, ligand = data\n",
    "        target, ligand = target.to(device), ligand.to(device)\n",
    "        atom_labels = torch.argmax(ligand.x, dim=1, keepdim=False)\n",
    "        bond_labels = torch.argmax(ligand.edge_attr, dim=1, keepdim=False)\n",
    "\n",
    "        pi, sigma, mu, dist, atom_types, bond_types, batch = model(ligand, target)\n",
    "\n",
    "        mdn = mdn_loss_fn(pi, sigma, mu, dist)\n",
    "        mdn = mdn[torch.where(dist <= model.dist_threhold)[0]]\n",
    "        mdn = mdn.mean()\n",
    "        atom = F.cross_entropy(atom_types, atom_labels)\n",
    "        bond = F.cross_entropy(bond_types, bond_labels)\n",
    "        loss = mdn + (atom * aux_weight) + (bond * aux_weight)\n",
    "\n",
    "        total_loss += loss.item() * (ligand.batch.max().item() + 1)\n",
    "        mdn_loss += mdn.item() * (ligand.batch.max().item() + 1)\n",
    "        atom_loss += atom.item() * (ligand.batch.max().item() + 1)\n",
    "        bond_loss += bond.item() * (ligand.batch.max().item() + 1)\n",
    "\n",
    "    return total_loss / len(loader.dataset), mdn_loss / len(loader.dataset), atom_loss / len(\n",
    "        loader.dataset), bond_loss / len(loader.dataset)\n",
    "\n",
    "\n",
    "prev_test_total_loss = 1000\n",
    "for epoch in range(1, epochs + 1):\n",
    "    total_loss, mdn_loss, atom_loss, bond_loss = train()\n",
    "    if np.isinf(mdn_loss) or np.isnan(mdn_loss):\n",
    "        print('Inf ERROR')\n",
    "        break\n",
    "    test_total_loss, test_mdn_loss, test_atom_loss, test_bond_loss = test(db_complex_test)\n",
    "    scheduler.step()\n",
    "    losses.append(\n",
    "        [total_loss, mdn_loss, atom_loss, bond_loss, test_total_loss, test_mdn_loss, test_atom_loss,\n",
    "         test_bond_loss])\n",
    "\n",
    "    if test_mdn_loss <= prev_test_total_loss:\n",
    "        prev_test_total_loss = test_total_loss\n",
    "        torch.save(\n",
    "            {'epoch': epoch, 'model_state_dict': model.state_dict(), 'optimizer_state_dict': optimizer.state_dict(),\n",
    "             'rng_state': torch.get_rng_state(), 'total_loss': total_loss,\n",
    "             'mdn_loss': mdn_loss, 'atom_loss': atom_loss, 'bond_loss': bond_loss},\n",
    "            f'{model_name}_minTestLoss.chk')\n",
    "    l = pd.DataFrame(losses,\n",
    "                     columns=['total_loss', 'mdn_loss', 'atom_loss', 'bond_loss', 'test_total_loss',\n",
    "                              'test_mdn_loss',\n",
    "                              'test_atom_loss', 'test_bond_loss'])\n",
    "    l.to_csv(f'dist7_{model_name}_loss.csv')\n",
    "\n",
    "    print(\n",
    "        'Epoch: {:03d}, Total Loss: {:.3f}, Valid Loss: {:.3f}'.format(epoch, total_loss, test_total_loss))\n",
    "\n",
    "    if epoch % save_each == 0:\n",
    "        torch.save(\n",
    "            {'epoch': epoch, 'model_state_dict': model.state_dict(), 'optimizer_state_dict': optimizer.state_dict(),\n",
    "             'rng_state': torch.get_rng_state(), 'total_loss': total_loss,\n",
    "             'mdn_loss': mdn_loss, 'atom_loss': atom_loss, 'bond_loss': bond_loss},\n",
    "            f'{model_name}_epoch_{epoch}.chk')\n",
    "        l = pd.DataFrame(losses, columns=['total_loss', 'mdn_loss', 'atom_loss', 'bond_loss', 'test_total_loss',\n",
    "                                          'test_mdn_loss', 'test_atom_loss', 'test_bond_loss'])\n",
    "        l.to_csv(f'dist7_{model_name}_loss.csv')\n",
    "\n",
    "torch.save({'epoch': epoch, 'model_state_dict': model.state_dict(), 'optimizer_state_dict': optimizer.state_dict(),\n",
    "            'rng_state': torch.get_rng_state(), 'total_loss': total_loss,\n",
    "            'mdn_loss': mdn_loss, 'atom_loss': atom_loss, 'bond_loss': bond_loss},\n",
    "           f'{model_name}_epoch_{epoch}.chk')\n",
    "l = pd.DataFrame(losses,\n",
    "                 columns=['total_loss', 'mdn_loss', 'atom_loss', 'bond_loss', 'test_total_loss', 'test_mdn_loss',\n",
    "                          'test_atom_loss', 'test_bond_loss'])\n",
    "l.to_csv(f'{model_name}_loss.csv')\n",
    "l[['total_loss', 'test_total_loss']].plot()"
   ],
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
